Scenario Based questions:

1Q)Will the reducer work or not if you use “Limit 1” in any HiveQL query?
1Ans):General select query will not create any reducer task since no shuffling of data.


2Q)Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
2Ans):Default metastore configuration will not allow to create mutiple sessions.If try to create multiple sessions 


3Q)Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
3Ans): Since problem statement is to fetch data based on month it is ideal to divide the whole into chunks based on month and perform the query

step1:
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;

CREATE external TABLE transaction_details_temp 
(cust_id INT, 
amount FLOAT,  
country STRING)
partitioned by (month string) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',';

step2:insert data from  orignal table  to temp table 

insert into table transaction_details_temp partiton(month) select * from transaction_details_temp;

step3:Repair the table after insertion gets complete

msck repair table transaction_details_temp;

step 4: set execution engine as spark and perform the query on temp table which is partitoned would give result faster

set hive.execution.engine=spark;

select sum(amount) as revenue,month from transaction_details_temp group by month;




4Q)How can you add a new partition for the month December in the above partitioned table?
4Ans):Since the partition methodology is dynamic in metastore consistency check command will sync the partitons and create Dec partition

  
5Q)I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
5Ans):By setting the below or running below commands will resolve the issue.

set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;



6Q)Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
6Ans):
create table sample_csv
(id int,
first_name string,
last_name string,
email string,
gender string,
ip_address string)
row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
stored as textfile;

load data inpath '/temp' into table sample_csv;


7Q)Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

7Ans):we can use the SequenceFile format which will group these small files together to form a single sequence file which would improve the performance

step 1:
create table emp_table_seq(
empID int,
empname string,
Dep string,
role string)
row format delimited
fields terminated by ','
stored as sequencefile;

step 2:insert into table emp_table_seq select * from small_file_table




8Q)LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?

8Ans: The source path does not contain data to be loaded and is empty directory

9Q)Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
9Ans):Step 1: Take a new system; create a new username and password
Step 2: Install SSH and with the master node setup SSH connections
Step 3: Add ssh public_rsa id key to the authorized keys file
Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:
192.168.1.102 slave3.in slave3
Step 5: Start the DataNode on a new node
Step 6: Login to the new node like suhadoop or:

ssh -X hadoop@192.168.1.103
Step 7: Start HDFS of the newly added slave node by using the following command:

./bin/hadoop-daemon.sh start data node
Step 8: Check the output of the jps command on the new node


Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)

Ans:create external table ineuron_test.CUSTOMERS (
ID int,NAME string,AGE int,ADDRESS string,SALARY int)
row format delimited fields terminated by ',';

hdfs dfs -copyFromLocal /home/cloudera/custdata.csv  hdfs://quickstart.cloudera:8020/user/hive/warehouse/ineuron_test.db/customers/

ALTER TABLE ineuron_test.CUSTOMERS
SET TBLPROPERTIES ("skip.header.line.count"="1");

Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Ans:
create external table ineuron_test.ORDER (OID int,DATE date,CUSTOMER_ID int,AMOUNT int)
row format delimited 
fields terminated by ',';

hdfs dfs -copyFromLocal /home/cloudera/orderdata.csv  hdfs://quickstart.cloudera:8020/user/hive/warehouse/ineuron_test.db/order/

ALTER TABLE ineuron_test.ORDER
SET TBLPROPERTIES ("skip.header.line.count"="1");
 

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

Ans:
select * from ineuron_test.CUSTOMERS a inner join ineuron_test.ORDER b on b.CUSTOMER_ID=a.ID;

select * from ineuron_test.CUSTOMERS a left join ineuron_test.ORDER b on b.CUSTOMER_ID=a.ID;

select * from ineuron_test.CUSTOMERS a right join ineuron_test.ORDER b on b.CUSTOMER_ID=a.ID;

select * from ineuron_test.CUSTOMERS a full join ineuron_test.ORDER b on b.CUSTOMER_ID=a.ID;

BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset
ans:create external table ineuron_test.air_quality(
Date date,
Time string,
CO array<int>,
PT08_S1 int,
NMHC int,
C6H6 array<int>,
PT08_S2 int,
NOx int,
PT08_S3 int,
NO2 int,
PT08_S4 int,
PT08_S5 int,
T array<int>,
RH array<int>,
AH array<int>)
row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
with serdeproperties (
"separatorChar" = "\;",
"quoteChar" = "\"",
"escapeChar" = "\\"
)
stored as textfile
tblproperties ("skip.header.line.count" = "1");
 
2. try to place a data into table location
ans:hdfs dfs -copyFromLocal /home/cloudera/AirQualityUCI.csv  hdfs://quickstart.cloudera:8020/user/hive/warehouse/ineuron_test.db/air_quality/


3. Perform a select operation 
ans:select * from ineuron_test.air_quality limit 5;
 
4. Fetch the result of the select operation in your local as a csv file
ans:hive  -e "set hive.cli.print.header=true;select a.* from ineuron_test.air_quality a limit 5" | sed 's/[\t]/,/g' > airop.csv
or
hive -f airquality.sql > /home/cloudera/myassignment/airop2.csv

5. Perform group by operation 
ans:select sum(nmhc),date from ineuron_test.air_quality group by date ;

7. Perform filter operation at least 5 kinds of filter examples

ans:select sum(nmhc),date from ineuron_test.air_quality group by date having sum(nmhc)>0;

select count(*) from ineuron_test.air_quality where date='29/09/2004';

select * from ineuron_test.air_quality where date between '29/09/2004' and '30/10/2004';

select date  from ineuron_test.air_quality where nmhc in (select max(nmhc) from ineuron_test.air_quality);

select avg(NOx),date from ineuron_test.air_quality where date>'29/09/2004' and NOx>0 group by date  ;


8. show and example of regex operation
ans:select regexp_replace('HA^G^FER$JY',"\\^","\\$")

9. alter table operation
ans: ALTER TABLE ineuron_test.air_quality SET TBLPROPERTIES('EXTERNAL'='FALSE');

10 . drop table operation
ans:drop table ineuron_test.air_quality;

12 . order by operation 
ans:select * from ineuron_test.air_quality order by date;

13 . where clause operations you have to perform 
ans:select count(*) from ineuron_test.air_quality where date='29/09/2004';

14 . sorting operation you have to perform . 
ans:select * from ineuron_test.air_quality sort by date;

15 . distinct operation you have to perform .
ans:select distinct nmhc from ineuron_test.air_quality;

16 . like an operation you have to perform
ans:select nmhc from ineuron_test.air_quality where nmhc like '%7';

17 . union operation you have to perform 
ans:select * from ineuron_test.air_quality where date='29/09/2004'
union all
select *from ineuron_test.air_quality where nmhc>0;

18 . table view operation you have to perform 
ans:create view ineuron_test.air_quality_vw as select * from ineuron_test.air_quality;






hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations
